# -*- coding: utf-8 -*-
"""MLP_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WC6i6SQiBJZ8-RBCym92okH24Bgvy21l
"""

import numpy as np
import pdb
import os
from tqdm import tqdm
from matplotlib import pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.autograd import Variable
from torch.utils.data import Dataset, DataLoader
from skimage.util import random_noise
import torchvision
from torchvision.datasets import FashionMNIST
from torchvision import transforms
from sklearn.model_selection import train_test_split
from utils import AverageMeter
from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay as cmd

class MLP(nn.Module):

    def __init__(self, n_classes=10):
        '''
        Define the initialization function of LeNet, this function defines
        the basic structure of the neural network
        '''

        super(MLP, self).__init__()
        self.fc1 = nn.Linear(784, 64)
        self.fc2 = nn.Linear(64, 64)
        #self.fc3 = nn.Linear(500, 16)
        #self.drop=nn.Dropout(.6)
        self.clf = nn.Linear(64, n_classes)

    def forward(self, x):
        x = x.view(-1, 784)
        x = F.relu(self.fc1(x))
        #x=self.drop(x)
        x = F.relu(self.fc2(x))
        #x=self.drop(x)
        #x = F.relu(self.fc3(x))
        out = self.clf(x)

        return out

def train_one_epoch(model, trainloader, optimizer, device,valloader):
    """ Training the model using the given dataloader for 1 epoch.

    Input: Model, Dataset, optimizer, 
    """

    model.train()
    avg_loss = AverageMeter("average-loss")
    #val_loss=0
    for batch_idx, (img, target) in enumerate(trainloader):
        #print(len(target))
        img = Variable(img)
        target = Variable(target).to(device)
        img_gauss=torch.tensor(random_noise(img, mode='gaussian', mean=0, var=0.05, clip=True)).float()
        img_sp=torch.tensor(random_noise(img, mode='speckle', mean=0, var=0.05, clip=True)).float()
        img_gauss=Variable(img_gauss).to(device)
        img_sp=Variable(img_sp).to(device)
        temp=[]
        temp.append(img.to(device))
        temp.append(img_gauss)
        temp.append(img_sp)
        #img.show()
        for i in range(3):
            #print('i:',i)
            # Zero out the gradients
            optimizer.zero_grad()

            # Forward Propagation
            out = model(temp[i])
            loss = F.cross_entropy(out, target)

            # backward propagation
            loss.backward()
            avg_loss.update(loss, temp[i].shape[0])

            # Update the model parameters
            optimizer.step()
    val_loss,acc,a,b=test(model,valloader,0)
    
    return avg_loss.avg,val_loss,acc.item()

def test(model, testloader,flag):
    total=0
    correct=0
    model.eval()
    #model.to('cpu')
    avg_loss = AverageMeter("average-loss")

    y_gt = []
    y_pred_label = []

    for batch_idx, (img, y_true) in enumerate(testloader):
        img = Variable(img)#.to(device)
        y_true = Variable(y_true)#.to(device)
        if use_cuda and torch.cuda.is_available():
          img = img.cuda()
          y_true = y_true.cuda()

        out = model(img)
        y_pred = F.softmax(out, dim=1)
        y_pred_label_tmp = torch.argmax(y_pred, dim=1)

        loss = F.cross_entropy(out, y_true)
        avg_loss.update(loss, img.shape[0])

        # Add the labels
        if(flag==1):
          y_gt+= list(y_true.cpu().numpy())
          y_pred_label += list(y_pred_label_tmp.cpu().numpy())
        
        total+=y_true.size(0)
        correct+=(y_pred_label_tmp==y_true).sum()
    #model.to('cuda:0')
    return avg_loss.avg,(100 * correct / total),y_gt,y_pred_label

from google.colab import drive
drive.mount('/content/drive')

if __name__ == "__main__":
    trans_img = transforms.Compose([transforms.ToTensor()])
    dataset = FashionMNIST("./data/", train=True, transform=trans_img, download=True)
    use_cuda=False



number_epochs = 50
device = torch.device('cpu')  # Replace with torch.device("cuda:0") if you want to train on GPU
model = MLP(10).to(device)

train_set,val_set=train_test_split(dataset,test_size=.20,random_state=7)

trainloader = DataLoader(train_set, batch_size=100, shuffle=True)
valloader = DataLoader(val_set, batch_size=100, shuffle=True)

#print(len(train_set))
#print(dataset.data.size())

optimizer = optim.Adam(model.parameters(), lr=0.001)
min_val_loss_epoch=(0,0,5) #acc,epoch,loss
track_loss = []
track_val_loss=[]
track_acc=[]
for i in tqdm(range(number_epochs)):
    loss,val_loss,acc = train_one_epoch(model, trainloader, optimizer, device,valloader)
    track_loss.append(loss.item())
    track_val_loss.append(val_loss.item())
    track_acc.append(acc)
    print('epoch:',i,'loss:',loss.item(),'val loss:',val_loss.item())
    if(acc>min_val_loss_epoch[0]):
      min_val_loss_epoch=(acc,i,val_loss.item())
      torch.save(model.state_dict(), "/content/drive/My Drive/MLP.pt")
      print('updated min:',val_loss.item(),'epoch:',i,'acc:',acc)
    #break;

#torch.save(model.state_dict(), "./models/MLP.pt")

plt.figure()
plt.plot(track_loss,'r')
plt.plot(track_val_loss,'g')
#plt.plot(track_acc,'b')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.title("training(red), val-loss(green): MLP")
#plt.title("training-loss-ConvNet")
plt.savefig("/content/drive/My Drive/training_MLP.jpg")

plt.figure()
plt.plot(track_loss,'b')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.title("training_loss:MLP")
#plt.title("training-loss-ConvNet")
plt.savefig("/content/drive/My Drive/rajat_training_MLP.jpg")

plt.figure()
plt.plot(track_acc,'b')
plt.title("Val_accuracy:MLP")
plt.savefig("/content/drive/My Drive/training_MLP_acc.jpg")

loss_val,acc_val,a,b=test(model,valloader,0)
loss_train,acc_train,a,b=test(model,trainloader,0)
print('accuracy on val data:',acc_val.item())
print('accuracy on train data:',acc_train.item())
print('loss on val data:',loss_val.item())
print('loss on train data:',loss_train.item())
print('best val loss epochs:',min_val_loss_epoch[1],'acc:',min_val_loss_epoch[0])

test_dataset = FashionMNIST("./data/", train=False, transform=trans_img, download=True)

test_loader= DataLoader(test_dataset,batch_size=500,shuffle=True)

bestModel=MLP(10).to(device)
bestModel.load_state_dict(torch.load("/content/drive/My Drive/MLP.pt"))

loss_test,acc_test,a,b=test(model,test_loader,0)

print('accuracy on test data:',acc_test.item())
print('loss on test data:',loss_test.item())

loss_test,acc_test,a,b=test(bestModel,test_loader,1)

print('accuracy on test data:',acc_test.item())
print('loss on test data:',loss_test.item())

#print(len(a))
display_labels=list(range(10))

np.set_printoptions(formatter={'float_kind':'{:f}'.format})
cm=confusion_matrix(a,b,normalize='all')
#print(cm)

#disp = cmd(confusion_matrix=cm,display_labels=display_labels)
#disp.plot(include_values=True,cmap='viridis', ax=None, xticks_rotation='horizontal',values_format=None)

import seaborn as sn
import pandas as pd

#cm=np.array(cm,dtype='float32')
#cm=float(cm)

df_cm = pd.DataFrame(cm, index = display_labels,
                  columns = display_labels)
#df_cm.round(2)
plt.figure(figsize = (10,7))
sn.heatmap(df_cm, annot=True)
plt.savefig("/content/drive/My Drive/confusion_MLP.jpg")

